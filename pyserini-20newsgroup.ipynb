{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import pysearch\n",
    "from pyserini.index import pyutils\n",
    "from pyserini.analysis.pyanalysis import get_lucene_analyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnson/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, defaultdict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 11314 documents in 20 categories\n",
      "test: 7532 documents in 20 categories\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data_subset_names = ['train', 'test']\n",
    "data = {}\n",
    "\n",
    "for data_subset_name in data_subset_names:\n",
    "    data_subset = fetch_20newsgroups(subset=data_subset_name)\n",
    "    data[data_subset_name] = data_subset\n",
    "    num_docs = len(data_subset.data)\n",
    "    num_categories = len(data_subset.target_names)\n",
    "    print(f\"{data_subset_name}: {num_docs} documents in {num_categories} categories\")\n",
    "\n",
    "# find data @ ~/scikit_learn_data/20news-bydate_py3.pkz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import pyutils\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class AnseriniTwentyNewsgroupTfidfVectorizer:\n",
    "    def __init__(self, index_path,  train_data, test_data):\n",
    "        self.index_utils = pyutils.IndexReaderUtils(index_path)\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.train_size = len(train_data.data)\n",
    "        self.test_size = len(test_data.data)\n",
    "        self.get_unique_terms()\n",
    "\n",
    "    def get_unique_terms(self):\n",
    "        unique_words = set()\n",
    "        self.terms_df = {}\n",
    "        for term in self.index_utils.terms():\n",
    "            unique_words.add(term.term)\n",
    "            self.terms_df[term.term] = term.df\n",
    "\n",
    "        self.word_to_index = {}\n",
    "        for index, term in enumerate(unique_words):\n",
    "            self.word_to_index[term] = index\n",
    "        self.num_unique_words = len(unique_words)\n",
    "        print(f'Found {self.num_unique_words} unique words')\n",
    "\n",
    "    def get_doc_id(self, filename):\n",
    "        return filename.split('/')[-1]\n",
    "\n",
    "    def get_train_vectors(self):\n",
    "        features = self.get_vectors(self.train_data.filenames)\n",
    "        return features, self.train_data.target\n",
    "\n",
    "    def get_test_vectors(self):\n",
    "        features = self.get_vectors(self.test_data.filenames)\n",
    "        return features, self.test_data.target\n",
    "\n",
    "    def get_vectors(self, filenames):\n",
    "        matrix_row, matrix_col, matrix_data = [], [], []\n",
    "\n",
    "        for index, filename in enumerate(filenames):\n",
    "            doc_id = self.get_doc_id(filename)\n",
    "            if index % 2000 == 0:\n",
    "                print(f'Vectorizing: {index}/{len(filenames)}')\n",
    "\n",
    "            # Term Frequency\n",
    "            tf = self.index_utils.get_document_vector(doc_id)\n",
    "\n",
    "            # Inverse Document Frequency\n",
    "            df = {t: math.log(\n",
    "                self.num_unique_words / self.terms_df[t]) for t in tf}\n",
    "\n",
    "            # Multiplication\n",
    "            total_num_terms_in_doc = sum(tf.values())\n",
    "            tfidf = {t: df[t] * tf[t] / total_num_terms_in_doc for t in tf}\n",
    "\n",
    "            # Convert from dict to sparse matrix\n",
    "            for term in tfidf:\n",
    "                i = self.word_to_index[term]\n",
    "                matrix_row.append(index)\n",
    "                matrix_col.append(i)\n",
    "                matrix_data.append(tfidf[term])\n",
    "\n",
    "        return csr_matrix((matrix_data, (matrix_row, matrix_col)), shape=(\n",
    "            len(filenames), self.num_unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 165633 unique words\n"
     ]
    }
   ],
   "source": [
    "vectorizer = AnseriniTwentyNewsgroupTfidfVectorizer(\n",
    "    './20-newsgroup/lucene-index.20newsgroup.pos+docvectors+raw',\n",
    "    data['train'],\n",
    "    data['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing: 0/11314\n",
      "Vectorizing: 2000/11314\n",
      "Vectorizing: 4000/11314\n",
      "Vectorizing: 6000/11314\n",
      "Vectorizing: 8000/11314\n",
      "Vectorizing: 10000/11314\n"
     ]
    }
   ],
   "source": [
    "features, labels = vectorizer.get_train_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing: 0/7532\n",
      "Vectorizing: 2000/7532\n",
      "Vectorizing: 4000/7532\n",
      "Vectorizing: 6000/7532\n"
     ]
    }
   ],
   "source": [
    "test_vectors, test_target = vectorizer.get_test_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5935516287791174"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(test_vectors)\n",
    "metrics.f1_score(test_target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
